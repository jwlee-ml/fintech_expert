{"cells":[{"cell_type":"markdown","source":["# Timeseries classification with a Transformer model\n","\n","**Author:** [Theodoros Ntakouris](https://github.com/ntakouris)<br>\n","**Date created:** 2021/06/25<br>\n","**Last modified:** 2021/08/05<br>\n","**Description:** This notebook demonstrates how to do timeseries classification using a Transformer model."],"metadata":{"id":"UJ9KQrnx5hh2"}},{"cell_type":"markdown","source":["## Introduction\n","\n","This is the Transformer architecture from\n","[Attention Is All You Need](https://arxiv.org/abs/1706.03762),\n","applied to timeseries instead of natural language.\n","\n","This example requires TensorFlow 2.4 or higher.\n","\n","## Load the dataset\n","\n","We are going to use the same dataset and preprocessing as the\n","[TimeSeries Classification from Scratch](https://keras.io/examples/timeseries/timeseries_classification_from_scratch)\n","example."],"metadata":{"id":"Kw6KoCnH5hh3"}},{"cell_type":"code","execution_count":1,"source":["import numpy as np\n","\n","\n","def readucr(filename):\n","    data = np.loadtxt(filename, delimiter=\"\\t\")\n","    y = data[:, 0]\n","    x = data[:, 1:]\n","    return x, y.astype(int)\n","\n","\n","root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n","\n","x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n","x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n","\n","x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n","x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n","\n","n_classes = len(np.unique(y_train))\n","\n","idx = np.random.permutation(len(x_train))\n","x_train = x_train[idx]\n","y_train = y_train[idx]\n","\n","y_train[y_train == -1] = 0\n","y_test[y_test == -1] = 0"],"outputs":[],"metadata":{"id":"GmnfCqOU5hh3","executionInfo":{"status":"ok","timestamp":1667001624541,"user_tz":-540,"elapsed":2270,"user":{"displayName":"JinWon Lee (DeepTube)","userId":"09449089977781029916"}}}},{"cell_type":"markdown","source":["## Build the model\n","\n","Our model processes a tensor of shape `(batch size, sequence length, features)`,\n","where `sequence length` is the number of time steps and `features` is each input\n","timeseries.\n","\n","You can replace your classification RNN layers with this one: the\n","inputs are fully compatible!"],"metadata":{"id":"CAa9UCDj5hh4"}},{"cell_type":"code","execution_count":2,"source":["from tensorflow import keras\n","from tensorflow.keras import layers"],"outputs":[],"metadata":{"id":"XqgBHmO35hh4","executionInfo":{"status":"ok","timestamp":1667001635682,"user_tz":-540,"elapsed":2895,"user":{"displayName":"JinWon Lee (DeepTube)","userId":"09449089977781029916"}}}},{"cell_type":"markdown","source":["We include residual connections, layer normalization, and dropout.\n","The resulting layer can be stacked multiple times.\n","\n","The projection layers are implemented through `keras.layers.Conv1D`."],"metadata":{"id":"_BBHK-Rp5hh4"}},{"cell_type":"code","execution_count":3,"source":["\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Normalization and Attention\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    return x + res\n"],"outputs":[],"metadata":{"id":"muKvXaaD5hh4","executionInfo":{"status":"ok","timestamp":1667001635682,"user_tz":-540,"elapsed":7,"user":{"displayName":"JinWon Lee (DeepTube)","userId":"09449089977781029916"}}}},{"cell_type":"markdown","source":["The main part of our model is now complete. We can stack multiple of those\n","`transformer_encoder` blocks and we can also proceed to add the final\n","Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n","layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n","our model down to a vector of features for each data point in the current\n","batch. A common way to achieve this is to use a pooling layer. For\n","this example, a `GlobalAveragePooling1D` layer is sufficient."],"metadata":{"id":"te4E2bbF5hh5"}},{"cell_type":"code","execution_count":4,"source":["\n","def build_model(\n","    input_shape,\n","    head_size,\n","    num_heads,\n","    ff_dim,\n","    num_transformer_blocks,\n","    mlp_units,\n","    dropout=0,\n","    mlp_dropout=0,\n","):\n","    inputs = keras.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = layers.Dense(dim, activation=\"relu\")(x)\n","        x = layers.Dropout(mlp_dropout)(x)\n","    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n","    return keras.Model(inputs, outputs)\n"],"outputs":[],"metadata":{"id":"I2fFrs0B5hh5","executionInfo":{"status":"ok","timestamp":1667001635683,"user_tz":-540,"elapsed":7,"user":{"displayName":"JinWon Lee (DeepTube)","userId":"09449089977781029916"}}}},{"cell_type":"markdown","source":["## Train and evaluate"],"metadata":{"id":"5BChxdZz5hh-"}},{"cell_type":"code","execution_count":5,"source":["input_shape = x_train.shape[1:]\n","\n","model = build_model(\n","    input_shape,\n","    head_size=256,\n","    num_heads=4,\n","    ff_dim=4,\n","    num_transformer_blocks=4,\n","    mlp_units=[128],\n","    mlp_dropout=0.4,\n","    dropout=0.25,\n",")\n","\n","model.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n","    metrics=[\"sparse_categorical_accuracy\"],\n",")\n","model.summary()\n","\n","callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n","\n","model.fit(\n","    x_train,\n","    y_train,\n","    validation_split=0.2,\n","    epochs=200,\n","    batch_size=64,\n","    callbacks=callbacks,\n",")\n","\n","model.evaluate(x_test, y_test, verbose=1)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 500, 1)]     0           []                               \n","                                                                                                  \n"," layer_normalization (LayerNorm  (None, 500, 1)      2           ['input_1[0][0]']                \n"," alization)                                                                                       \n","                                                                                                  \n"," multi_head_attention (MultiHea  (None, 500, 1)      7169        ['layer_normalization[0][0]',    \n"," dAttention)                                                      'layer_normalization[0][0]']    \n","                                                                                                  \n"," dropout (Dropout)              (None, 500, 1)       0           ['multi_head_attention[0][0]']   \n","                                                                                                  \n"," tf.__operators__.add (TFOpLamb  (None, 500, 1)      0           ['dropout[0][0]',                \n"," da)                                                              'input_1[0][0]']                \n","                                                                                                  \n"," layer_normalization_1 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add[0][0]']   \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv1d (Conv1D)                (None, 500, 4)       8           ['layer_normalization_1[0][0]']  \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 500, 4)       0           ['conv1d[0][0]']                 \n","                                                                                                  \n"," conv1d_1 (Conv1D)              (None, 500, 1)       5           ['dropout_1[0][0]']              \n","                                                                                                  \n"," tf.__operators__.add_1 (TFOpLa  (None, 500, 1)      0           ['conv1d_1[0][0]',               \n"," mbda)                                                            'tf.__operators__.add[0][0]']   \n","                                                                                                  \n"," layer_normalization_2 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_1[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_1 (MultiH  (None, 500, 1)      7169        ['layer_normalization_2[0][0]',  \n"," eadAttention)                                                    'layer_normalization_2[0][0]']  \n","                                                                                                  \n"," dropout_2 (Dropout)            (None, 500, 1)       0           ['multi_head_attention_1[0][0]'] \n","                                                                                                  \n"," tf.__operators__.add_2 (TFOpLa  (None, 500, 1)      0           ['dropout_2[0][0]',              \n"," mbda)                                                            'tf.__operators__.add_1[0][0]'] \n","                                                                                                  \n"," layer_normalization_3 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_2[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv1d_2 (Conv1D)              (None, 500, 4)       8           ['layer_normalization_3[0][0]']  \n","                                                                                                  \n"," dropout_3 (Dropout)            (None, 500, 4)       0           ['conv1d_2[0][0]']               \n","                                                                                                  \n"," conv1d_3 (Conv1D)              (None, 500, 1)       5           ['dropout_3[0][0]']              \n","                                                                                                  \n"," tf.__operators__.add_3 (TFOpLa  (None, 500, 1)      0           ['conv1d_3[0][0]',               \n"," mbda)                                                            'tf.__operators__.add_2[0][0]'] \n","                                                                                                  \n"," layer_normalization_4 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_3[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_2 (MultiH  (None, 500, 1)      7169        ['layer_normalization_4[0][0]',  \n"," eadAttention)                                                    'layer_normalization_4[0][0]']  \n","                                                                                                  \n"," dropout_4 (Dropout)            (None, 500, 1)       0           ['multi_head_attention_2[0][0]'] \n","                                                                                                  \n"," tf.__operators__.add_4 (TFOpLa  (None, 500, 1)      0           ['dropout_4[0][0]',              \n"," mbda)                                                            'tf.__operators__.add_3[0][0]'] \n","                                                                                                  \n"," layer_normalization_5 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_4[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv1d_4 (Conv1D)              (None, 500, 4)       8           ['layer_normalization_5[0][0]']  \n","                                                                                                  \n"," dropout_5 (Dropout)            (None, 500, 4)       0           ['conv1d_4[0][0]']               \n","                                                                                                  \n"," conv1d_5 (Conv1D)              (None, 500, 1)       5           ['dropout_5[0][0]']              \n","                                                                                                  \n"," tf.__operators__.add_5 (TFOpLa  (None, 500, 1)      0           ['conv1d_5[0][0]',               \n"," mbda)                                                            'tf.__operators__.add_4[0][0]'] \n","                                                                                                  \n"," layer_normalization_6 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_5[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," multi_head_attention_3 (MultiH  (None, 500, 1)      7169        ['layer_normalization_6[0][0]',  \n"," eadAttention)                                                    'layer_normalization_6[0][0]']  \n","                                                                                                  \n"," dropout_6 (Dropout)            (None, 500, 1)       0           ['multi_head_attention_3[0][0]'] \n","                                                                                                  \n"," tf.__operators__.add_6 (TFOpLa  (None, 500, 1)      0           ['dropout_6[0][0]',              \n"," mbda)                                                            'tf.__operators__.add_5[0][0]'] \n","                                                                                                  \n"," layer_normalization_7 (LayerNo  (None, 500, 1)      2           ['tf.__operators__.add_6[0][0]'] \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv1d_6 (Conv1D)              (None, 500, 4)       8           ['layer_normalization_7[0][0]']  \n","                                                                                                  \n"," dropout_7 (Dropout)            (None, 500, 4)       0           ['conv1d_6[0][0]']               \n","                                                                                                  \n"," conv1d_7 (Conv1D)              (None, 500, 1)       5           ['dropout_7[0][0]']              \n","                                                                                                  \n"," tf.__operators__.add_7 (TFOpLa  (None, 500, 1)      0           ['conv1d_7[0][0]',               \n"," mbda)                                                            'tf.__operators__.add_6[0][0]'] \n","                                                                                                  \n"," global_average_pooling1d (Glob  (None, 500)         0           ['tf.__operators__.add_7[0][0]'] \n"," alAveragePooling1D)                                                                              \n","                                                                                                  \n"," dense (Dense)                  (None, 128)          64128       ['global_average_pooling1d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n"," dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 2)            258         ['dropout_8[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 93,130\n","Trainable params: 93,130\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/200\n","45/45 [==============================] - 16s 92ms/step - loss: 1.0842 - sparse_categorical_accuracy: 0.5288 - val_loss: 0.7591 - val_sparse_categorical_accuracy: 0.5479\n","Epoch 2/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.8568 - sparse_categorical_accuracy: 0.5806 - val_loss: 0.6714 - val_sparse_categorical_accuracy: 0.6200\n","Epoch 3/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.7953 - sparse_categorical_accuracy: 0.5941 - val_loss: 0.6315 - val_sparse_categorical_accuracy: 0.6546\n","Epoch 4/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.7324 - sparse_categorical_accuracy: 0.6281 - val_loss: 0.6046 - val_sparse_categorical_accuracy: 0.6824\n","Epoch 5/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.6877 - sparse_categorical_accuracy: 0.6413 - val_loss: 0.5866 - val_sparse_categorical_accuracy: 0.6671\n","Epoch 6/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.6740 - sparse_categorical_accuracy: 0.6545 - val_loss: 0.5696 - val_sparse_categorical_accuracy: 0.6935\n","Epoch 7/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.6294 - sparse_categorical_accuracy: 0.6792 - val_loss: 0.5612 - val_sparse_categorical_accuracy: 0.7032\n","Epoch 8/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.6006 - sparse_categorical_accuracy: 0.6965 - val_loss: 0.5536 - val_sparse_categorical_accuracy: 0.7046\n","Epoch 9/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.5746 - sparse_categorical_accuracy: 0.7038 - val_loss: 0.5399 - val_sparse_categorical_accuracy: 0.7240\n","Epoch 10/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.5417 - sparse_categorical_accuracy: 0.7267 - val_loss: 0.5407 - val_sparse_categorical_accuracy: 0.7254\n","Epoch 11/200\n","45/45 [==============================] - 4s 80ms/step - loss: 0.5474 - sparse_categorical_accuracy: 0.7267 - val_loss: 0.5312 - val_sparse_categorical_accuracy: 0.7393\n","Epoch 12/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.5385 - sparse_categorical_accuracy: 0.7267 - val_loss: 0.5281 - val_sparse_categorical_accuracy: 0.7379\n","Epoch 13/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.5089 - sparse_categorical_accuracy: 0.7528 - val_loss: 0.5251 - val_sparse_categorical_accuracy: 0.7434\n","Epoch 14/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4889 - sparse_categorical_accuracy: 0.7639 - val_loss: 0.5188 - val_sparse_categorical_accuracy: 0.7448\n","Epoch 15/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4761 - sparse_categorical_accuracy: 0.7719 - val_loss: 0.5165 - val_sparse_categorical_accuracy: 0.7503\n","Epoch 16/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4790 - sparse_categorical_accuracy: 0.7771 - val_loss: 0.5084 - val_sparse_categorical_accuracy: 0.7628\n","Epoch 17/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4639 - sparse_categorical_accuracy: 0.7795 - val_loss: 0.5055 - val_sparse_categorical_accuracy: 0.7573\n","Epoch 18/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4521 - sparse_categorical_accuracy: 0.7872 - val_loss: 0.5002 - val_sparse_categorical_accuracy: 0.7587\n","Epoch 19/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4534 - sparse_categorical_accuracy: 0.7972 - val_loss: 0.4988 - val_sparse_categorical_accuracy: 0.7601\n","Epoch 20/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4320 - sparse_categorical_accuracy: 0.8031 - val_loss: 0.4926 - val_sparse_categorical_accuracy: 0.7684\n","Epoch 21/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4244 - sparse_categorical_accuracy: 0.8118 - val_loss: 0.4944 - val_sparse_categorical_accuracy: 0.7531\n","Epoch 22/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4061 - sparse_categorical_accuracy: 0.8177 - val_loss: 0.4837 - val_sparse_categorical_accuracy: 0.7725\n","Epoch 23/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3970 - sparse_categorical_accuracy: 0.8205 - val_loss: 0.4798 - val_sparse_categorical_accuracy: 0.7753\n","Epoch 24/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.4041 - sparse_categorical_accuracy: 0.8191 - val_loss: 0.4819 - val_sparse_categorical_accuracy: 0.7739\n","Epoch 25/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3903 - sparse_categorical_accuracy: 0.8313 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.7795\n","Epoch 26/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3859 - sparse_categorical_accuracy: 0.8306 - val_loss: 0.4750 - val_sparse_categorical_accuracy: 0.7781\n","Epoch 27/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3741 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.4712 - val_sparse_categorical_accuracy: 0.7795\n","Epoch 28/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8330 - val_loss: 0.4708 - val_sparse_categorical_accuracy: 0.7781\n","Epoch 29/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3674 - sparse_categorical_accuracy: 0.8458 - val_loss: 0.4676 - val_sparse_categorical_accuracy: 0.7836\n","Epoch 30/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3678 - sparse_categorical_accuracy: 0.8455 - val_loss: 0.4648 - val_sparse_categorical_accuracy: 0.7864\n","Epoch 31/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3558 - sparse_categorical_accuracy: 0.8479 - val_loss: 0.4615 - val_sparse_categorical_accuracy: 0.7920\n","Epoch 32/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3530 - sparse_categorical_accuracy: 0.8562 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.7933\n","Epoch 33/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3366 - sparse_categorical_accuracy: 0.8576 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.7933\n","Epoch 34/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3413 - sparse_categorical_accuracy: 0.8562 - val_loss: 0.4539 - val_sparse_categorical_accuracy: 0.8003\n","Epoch 35/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3447 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.4544 - val_sparse_categorical_accuracy: 0.7933\n","Epoch 36/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3352 - sparse_categorical_accuracy: 0.8642 - val_loss: 0.4490 - val_sparse_categorical_accuracy: 0.7920\n","Epoch 37/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3224 - sparse_categorical_accuracy: 0.8681 - val_loss: 0.4427 - val_sparse_categorical_accuracy: 0.7989\n","Epoch 38/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3248 - sparse_categorical_accuracy: 0.8649 - val_loss: 0.4414 - val_sparse_categorical_accuracy: 0.7961\n","Epoch 39/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.8660 - val_loss: 0.4444 - val_sparse_categorical_accuracy: 0.7975\n","Epoch 40/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3152 - sparse_categorical_accuracy: 0.8719 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.7961\n","Epoch 41/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3178 - sparse_categorical_accuracy: 0.8705 - val_loss: 0.4379 - val_sparse_categorical_accuracy: 0.8003\n","Epoch 42/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3124 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.8017\n","Epoch 43/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.4357 - val_sparse_categorical_accuracy: 0.8072\n","Epoch 44/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.3022 - sparse_categorical_accuracy: 0.8785 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8100\n","Epoch 45/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2963 - sparse_categorical_accuracy: 0.8840 - val_loss: 0.4289 - val_sparse_categorical_accuracy: 0.8086\n","Epoch 46/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.8865 - val_loss: 0.4297 - val_sparse_categorical_accuracy: 0.8031\n","Epoch 47/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2751 - sparse_categorical_accuracy: 0.9021 - val_loss: 0.4259 - val_sparse_categorical_accuracy: 0.8058\n","Epoch 48/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2820 - sparse_categorical_accuracy: 0.8955 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8072\n","Epoch 49/200\n","45/45 [==============================] - 4s 78ms/step - loss: 0.2719 - sparse_categorical_accuracy: 0.8986 - val_loss: 0.4254 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 50/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2779 - sparse_categorical_accuracy: 0.8997 - val_loss: 0.4239 - val_sparse_categorical_accuracy: 0.8114\n","Epoch 51/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2698 - sparse_categorical_accuracy: 0.9017 - val_loss: 0.4234 - val_sparse_categorical_accuracy: 0.8086\n","Epoch 52/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2743 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.8128\n","Epoch 53/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2563 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.4200 - val_sparse_categorical_accuracy: 0.8114\n","Epoch 54/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2640 - sparse_categorical_accuracy: 0.9038 - val_loss: 0.4188 - val_sparse_categorical_accuracy: 0.8141\n","Epoch 55/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2596 - sparse_categorical_accuracy: 0.9038 - val_loss: 0.4203 - val_sparse_categorical_accuracy: 0.8114\n","Epoch 56/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2649 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.4139 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 57/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2480 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.4201 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 58/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2479 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8197\n","Epoch 59/200\n","45/45 [==============================] - 4s 80ms/step - loss: 0.2487 - sparse_categorical_accuracy: 0.9094 - val_loss: 0.4135 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 60/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2468 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.4157 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 61/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2418 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.4116 - val_sparse_categorical_accuracy: 0.8169\n","Epoch 62/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2364 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.4129 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 63/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2360 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8141\n","Epoch 64/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2326 - sparse_categorical_accuracy: 0.9135 - val_loss: 0.4121 - val_sparse_categorical_accuracy: 0.8141\n","Epoch 65/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2329 - sparse_categorical_accuracy: 0.9156 - val_loss: 0.4142 - val_sparse_categorical_accuracy: 0.8211\n","Epoch 66/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2229 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.4110 - val_sparse_categorical_accuracy: 0.8197\n","Epoch 67/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2277 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.4076 - val_sparse_categorical_accuracy: 0.8141\n","Epoch 68/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2359 - sparse_categorical_accuracy: 0.9056 - val_loss: 0.4061 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 69/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2207 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.8197\n","Epoch 70/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2121 - sparse_categorical_accuracy: 0.9257 - val_loss: 0.4098 - val_sparse_categorical_accuracy: 0.8128\n","Epoch 71/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2158 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.4096 - val_sparse_categorical_accuracy: 0.8169\n","Epoch 72/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2095 - sparse_categorical_accuracy: 0.9253 - val_loss: 0.4062 - val_sparse_categorical_accuracy: 0.8211\n","Epoch 73/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2107 - sparse_categorical_accuracy: 0.9281 - val_loss: 0.4078 - val_sparse_categorical_accuracy: 0.8225\n","Epoch 74/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2138 - sparse_categorical_accuracy: 0.9236 - val_loss: 0.4017 - val_sparse_categorical_accuracy: 0.8197\n","Epoch 75/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2061 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.8169\n","Epoch 76/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2006 - sparse_categorical_accuracy: 0.9316 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 77/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1954 - sparse_categorical_accuracy: 0.9372 - val_loss: 0.4014 - val_sparse_categorical_accuracy: 0.8169\n","Epoch 78/200\n","45/45 [==============================] - 4s 80ms/step - loss: 0.2075 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.4002 - val_sparse_categorical_accuracy: 0.8211\n","Epoch 79/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1982 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.4034 - val_sparse_categorical_accuracy: 0.8225\n","Epoch 80/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2000 - sparse_categorical_accuracy: 0.9257 - val_loss: 0.4046 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 81/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2020 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.4019 - val_sparse_categorical_accuracy: 0.8225\n","Epoch 82/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.2026 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.4010 - val_sparse_categorical_accuracy: 0.8211\n","Epoch 83/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1891 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.3968 - val_sparse_categorical_accuracy: 0.8252\n","Epoch 84/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1843 - sparse_categorical_accuracy: 0.9396 - val_loss: 0.4009 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 85/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1897 - sparse_categorical_accuracy: 0.9354 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 86/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1838 - sparse_categorical_accuracy: 0.9319 - val_loss: 0.3974 - val_sparse_categorical_accuracy: 0.8155\n","Epoch 87/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1866 - sparse_categorical_accuracy: 0.9368 - val_loss: 0.4016 - val_sparse_categorical_accuracy: 0.8197\n","Epoch 88/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1929 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.3983 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 89/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1836 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.4042 - val_sparse_categorical_accuracy: 0.8225\n","Epoch 90/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1833 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.3967 - val_sparse_categorical_accuracy: 0.8128\n","Epoch 91/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1829 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.3999 - val_sparse_categorical_accuracy: 0.8225\n","Epoch 92/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1719 - sparse_categorical_accuracy: 0.9462 - val_loss: 0.4010 - val_sparse_categorical_accuracy: 0.8141\n","Epoch 93/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1718 - sparse_categorical_accuracy: 0.9472 - val_loss: 0.4033 - val_sparse_categorical_accuracy: 0.8128\n","Epoch 94/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1704 - sparse_categorical_accuracy: 0.9465 - val_loss: 0.4018 - val_sparse_categorical_accuracy: 0.8183\n","Epoch 95/200\n","45/45 [==============================] - 4s 79ms/step - loss: 0.1696 - sparse_categorical_accuracy: 0.9438 - val_loss: 0.3997 - val_sparse_categorical_accuracy: 0.8141\n","42/42 [==============================] - 1s 13ms/step - loss: 0.3703 - sparse_categorical_accuracy: 0.8447\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.370273232460022, 0.8446969985961914]"]},"metadata":{},"execution_count":5}],"metadata":{"id":"3dR_Y5g55hh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667001989294,"user_tz":-540,"elapsed":353617,"user":{"displayName":"JinWon Lee (DeepTube)","userId":"09449089977781029916"}},"outputId":"5c1c1702-5cfe-4534-8b4b-523cdb9f61f9"}},{"cell_type":"markdown","source":["## Conclusions\n","\n","In about 110-120 epochs (25s each on Colab), the model reaches a training\n","accuracy of ~0.95, validation accuracy of ~84 and a testing\n","accuracy of ~85, without hyperparameter tuning. And that is for a model\n","with less than 100k parameters. Of course, parameter count and accuracy could be\n","improved by a hyperparameter search and a more sophisticated learning rate\n","schedule, or a different optimizer.\n","\n","You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification)."],"metadata":{"id":"3pwLVlDC5hh-"}}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}